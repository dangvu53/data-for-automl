{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn2Clean Example: ANLI R1 Dataset\n",
    "\n",
    "This notebook demonstrates how to apply Learn2Clean to the ANLI R1 (Adversarial Natural Language Inference Round 1) dataset for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Learn2Clean with compatible package versions...\n",
      "/storage/nammt/autogluon/Learn2Clean/python-package\n",
      "\u001b[33mWARNING: Skipping learn2clean as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping learn2clean as it is not installed.\u001b[0m\n",
      "Obtaining file:///storage/nammt/autogluon/Learn2Clean/python-package\n",
      "Obtaining file:///storage/nammt/autogluon/Learn2Clean/python-package\n",
      "Installing collected packages: learn2clean\n",
      "  Running setup.py develop for learn2clean\n",
      "Installing collected packages: learn2clean\n",
      "  Running setup.py develop for learn2clean\n",
      "Successfully installed learn2clean\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/storage/nammt/autogluon/learn2clean_env/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Successfully installed learn2clean\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/storage/nammt/autogluon/learn2clean_env/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Installing compatible dependencies...\n",
      "Installing compatible dependencies...\n",
      "Collecting python-Levenshtein\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
      "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting fuzzywuzzy\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting Levenshtein==0.23.0\n",
      "Collecting Levenshtein==0.23.0\n",
      "  Downloading Levenshtein-0.23.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170 kB)\n",
      "\u001b[?25l  Downloading Levenshtein-0.23.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 170 kB 1.3 MB/s \n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0\n",
      "Collecting rapidfuzz<4.0.0,>=3.1.0\n",
      "  Downloading rapidfuzz-3.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |██                              | 204 kB 9.9 MB/s eta 0:00:01  Downloading rapidfuzz-3.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 9.9 MB/s eta 0:00:01     |███████████▋                    | 1.1 MB 9.9 MB/s eta 0:00:01     |███████████▋                    | 1.1 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein, fuzzywuzzy\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein, fuzzywuzzy\n",
      "Successfully installed Levenshtein-0.23.0 fuzzywuzzy-0.18.0 python-Levenshtein-0.23.0 rapidfuzz-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/storage/nammt/autogluon/learn2clean_env/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Successfully installed Levenshtein-0.23.0 fuzzywuzzy-0.18.0 python-Levenshtein-0.23.0 rapidfuzz-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/storage/nammt/autogluon/learn2clean_env/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "✓ Installed alternative string matching libraries\n",
      "✓ Using existing numpy, pandas, scikit-learn, scipy, matplotlib\n",
      "⚠ Skipping fancyimpute and string matching libraries due to version conflicts\n",
      "/storage/nammt/autogluon/Learn2Clean/examples\n",
      "\n",
      "✓ Learn2Clean installed with core functionality!\n",
      "Note: Some advanced features (fancy imputation, string similarity) may be limited\n",
      "✓ Installed alternative string matching libraries\n",
      "✓ Using existing numpy, pandas, scikit-learn, scipy, matplotlib\n",
      "⚠ Skipping fancyimpute and string matching libraries due to version conflicts\n",
      "/storage/nammt/autogluon/Learn2Clean/examples\n",
      "\n",
      "✓ Learn2Clean installed with core functionality!\n",
      "Note: Some advanced features (fancy imputation, string similarity) may be limited\n"
     ]
    }
   ],
   "source": [
    "# Install Learn2Clean with compatible versions - avoiding dependency conflicts\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Setting up Learn2Clean with compatible package versions...\")\n",
    "\n",
    "if os.path.exists('../python-package'):\n",
    "    %cd ../python-package\n",
    "    \n",
    "    # First uninstall any existing Learn2Clean\n",
    "    !pip uninstall -y learn2clean\n",
    "    \n",
    "    # Install Learn2Clean without dependencies to avoid conflicts\n",
    "    !pip install -e . --no-deps\n",
    "    \n",
    "    # Now install compatible versions of the dependencies we need\n",
    "    print(\"Installing compatible dependencies...\")\n",
    "    \n",
    "    # Install string matching libraries with fallback\n",
    "    try:\n",
    "        # Try alternative string matching that might compile better\n",
    "        !pip install python-Levenshtein fuzzywuzzy\n",
    "        print(\"✓ Installed alternative string matching libraries\")\n",
    "    except:\n",
    "        print(\"⚠ Warning: Advanced string matching not available, using basic alternatives\")\n",
    "    \n",
    "    # Install basic ML dependencies we already have\n",
    "    print(\"✓ Using existing numpy, pandas, scikit-learn, scipy, matplotlib\")\n",
    "    \n",
    "    # Skip problematic dependencies (fancyimpute, py_stringmatching, py_stringsimjoin)\n",
    "    print(\"⚠ Skipping fancyimpute and string matching libraries due to version conflicts\")\n",
    "    \n",
    "    %cd ../examples\n",
    "    \n",
    "    print(\"\\n✓ Learn2Clean installed with core functionality!\")\n",
    "    print(\"Note: Some advanced features (fancy imputation, string similarity) may be limited\")\n",
    "    \n",
    "else:\n",
    "    print(\"Learn2Clean python-package directory not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Research\\data-for-automl\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ANLI R1 dataset...\n",
      "ANLI R1 loaded: Train=16946, Val=1000, Test=1000\n"
     ]
    }
   ],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def load_anli_r1_dataset():\n",
    "    \"\"\"Load and prepare ANLI R1 dataset for text classification\"\"\"\n",
    "    print(\"Loading ANLI R1 dataset...\")\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"facebook/anli\")\n",
    "        \n",
    "        def prepare_anli_data(split_data):\n",
    "            data = []\n",
    "            for item in split_data:\n",
    "                # Combine premise and hypothesis for NLI\n",
    "                text_features = f\"[PREMISE] {item['premise']} [HYPOTHESIS] {item['hypothesis']}\"\n",
    "                \n",
    "                data.append({\n",
    "                    'text': text_features,\n",
    "                    'premise': item['premise'],\n",
    "                    'hypothesis': item['hypothesis'],\n",
    "                    'label': item['label']\n",
    "                })\n",
    "            return pd.DataFrame(data)\n",
    "        \n",
    "        train_df = prepare_anli_data(dataset['train_r1'])\n",
    "        val_df = prepare_anli_data(dataset['dev_r1'])\n",
    "        test_df = prepare_anli_data(dataset['test_r1'])\n",
    "\n",
    "        print(f\"ANLI R1 loaded: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "        return train_df, val_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ANLI: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the dataset\n",
    "train_df, val_df, test_df = load_anli_r1_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:\n",
      "Train: (16946, 4)\n",
      "Validation: (1000, 4)\n",
      "Test: (1000, 4)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PREMISE] The Parma trolleybus system (Italian...</td>\n",
       "      <td>The Parma trolleybus system (Italian: \"Rete fi...</td>\n",
       "      <td>The trolleybus system has over 2 urban routes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[PREMISE] Alexandra Lendon Bastedo (9 March 19...</td>\n",
       "      <td>Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...</td>\n",
       "      <td>Sharron Macready was a popular character throu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[PREMISE] Alexandra Lendon Bastedo (9 March 19...</td>\n",
       "      <td>Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...</td>\n",
       "      <td>Bastedo didn't keep any pets because of her vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PREMISE] Alexandra Lendon Bastedo (9 March 19...</td>\n",
       "      <td>Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...</td>\n",
       "      <td>Alexandra Bastedo was named by her mother.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[PREMISE] Alexandra Lendon Bastedo (9 March 19...</td>\n",
       "      <td>Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...</td>\n",
       "      <td>Bastedo cared for all the animals that inhabit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [PREMISE] The Parma trolleybus system (Italian...   \n",
       "1  [PREMISE] Alexandra Lendon Bastedo (9 March 19...   \n",
       "2  [PREMISE] Alexandra Lendon Bastedo (9 March 19...   \n",
       "3  [PREMISE] Alexandra Lendon Bastedo (9 March 19...   \n",
       "4  [PREMISE] Alexandra Lendon Bastedo (9 March 19...   \n",
       "\n",
       "                                             premise  \\\n",
       "0  The Parma trolleybus system (Italian: \"Rete fi...   \n",
       "1  Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...   \n",
       "2  Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...   \n",
       "3  Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...   \n",
       "4  Alexandra Lendon Bastedo (9 March 1946 – 12 Ja...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0      The trolleybus system has over 2 urban routes      0  \n",
       "1  Sharron Macready was a popular character throu...      1  \n",
       "2  Bastedo didn't keep any pets because of her vi...      1  \n",
       "3         Alexandra Bastedo was named by her mother.      1  \n",
       "4  Bastedo cared for all the animals that inhabit...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    7052\n",
      "0    5371\n",
      "2    4523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16946 entries, 0 to 16945\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        16946 non-null  object\n",
      " 1   premise     16946 non-null  object\n",
      " 2   hypothesis  16946 non-null  object\n",
      " 3   label       16946 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 529.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "if train_df is not None:\n",
    "    print(\"Dataset shape:\")\n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Validation: {val_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(train_df.head())\n",
    "    \n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(train_df['label'].value_counts())\n",
    "    \n",
    "    print(\"\\nColumn info:\")\n",
    "    print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare Data for Learn2Clean\n",
    "\n",
    "Learn2Clean works with CSV files, so we need to save our data and create a reader function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully!\n",
      "Train size: 16946\n",
      "Validation size: 1000\n",
      "Test size: 1000\n",
      "\n",
      "IMPORTANT: Train/val/test kept separate to avoid data leakage for AutoGluon!\n"
     ]
    }
   ],
   "source": [
    "# Create datasets directory if it doesn't exist\n",
    "os.makedirs('../datasets/anli_r1', exist_ok=True)\n",
    "\n",
    "# Save datasets as CSV files - KEEP TRAIN AND VALIDATION SEPARATE!\n",
    "if train_df is not None:\n",
    "    # Save train, validation, and test separately to avoid data leakage\n",
    "    train_df.to_csv('../datasets/anli_r1/anli_r1_train.csv', index=False, encoding='utf-8')\n",
    "    val_df.to_csv('../datasets/anli_r1/anli_r1_val.csv', index=False, encoding='utf-8')\n",
    "    test_df.to_csv('../datasets/anli_r1/anli_r1_test.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"Datasets saved successfully!\")\n",
    "    print(f\"Train size: {len(train_df)}\")\n",
    "    print(f\"Validation size: {len(val_df)}\")\n",
    "    print(f\"Test size: {len(test_df)}\")\n",
    "    print(\"\\nIMPORTANT: Train/val/test kept separate to avoid data leakage for AutoGluon!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset shape: (16946, 4)\n",
      "Columns: ['text', 'premise', 'hypothesis', 'label']\n",
      "\n",
      "Dataset split sizes:\n",
      "Train: 16946\n",
      "Validation: 1000\n",
      "Test: 1000\n"
     ]
    }
   ],
   "source": [
    "# Define dataset reader function for Learn2Clean\n",
    "def read_dataset(name):\n",
    "    \"\"\"Load datasets for Learn2Clean processing\"\"\"\n",
    "    import pandas as pd\n",
    "    if name == \"anli_r1\":\n",
    "        df = pd.read_csv('../datasets/anli_r1/anli_r1_train.csv', sep=',', encoding='utf-8')\n",
    "    elif name == \"anli_r1_val\":\n",
    "        df = pd.read_csv('../datasets/anli_r1/anli_r1_val.csv', sep=',', encoding='utf-8')\n",
    "    elif name == \"anli_r1_test\":\n",
    "        df = pd.read_csv('../datasets/anli_r1/anli_r1_test.csv', sep=',', encoding='utf-8')\n",
    "    else: \n",
    "        raise ValueError('Invalid dataset name')               \n",
    "    return df\n",
    "\n",
    "# Test the reader function\n",
    "test_load = read_dataset(\"anli_r1\")\n",
    "print(f\"Loaded train dataset shape: {test_load.shape}\")\n",
    "print(f\"Columns: {test_load.columns.tolist()}\")\n",
    "\n",
    "# Verify all splits\n",
    "print(f\"\\nDataset split sizes:\")\n",
    "print(f\"Train: {len(read_dataset('anli_r1'))}\")\n",
    "print(f\"Validation: {len(read_dataset('anli_r1_val'))}\")\n",
    "print(f\"Test: {len(read_dataset('anli_r1_test'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Profiling with Learn2Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling datasets\n",
      "    Attribute    Type  Num. Missing Values  Num. Unique Values Sknewness  Kurtosis\n",
      "0       label   int64                    0                   3  0.084282 -1.276195\n",
      "1        text  object                    0               16943       N/A       N/A\n",
      "2     premise  object                    0                2054       N/A       N/A\n",
      "3  hypothesis  object                    0               16941       N/A       N/A\n"
     ]
    }
   ],
   "source": [
    "# Add Learn2Clean to Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../python-package'))\n",
    "\n",
    "from learn2clean.loading import reader as rd \n",
    "from learn2clean.normalization import normalizer as nl \n",
    "import pandas as pd\n",
    "\n",
    "# Execute profiling function for ANLI R1 dataset\n",
    "rd.profile_summary(read_dataset('anli_r1'), plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable (label) distribution:\n",
      "label\n",
      "1    7052\n",
      "0    5371\n",
      "2    4523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target variable head:\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the target variable\n",
    "anli_data = read_dataset('anli_r1')\n",
    "print(\"Target variable (label) distribution:\")\n",
    "print(anli_data['label'].value_counts())\n",
    "print(\"\\nTarget variable head:\")\n",
    "print(anli_data['label'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Learn2Clean Data Processing\n",
    "\n",
    "Now we'll use Learn2Clean's Reader class to process the ANLI R1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading csv : anli_r1_train.csv ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'error_bad_lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Process ANLI R1 dataset - ONLY TRAIN DATA for Learn2Clean optimization\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# This avoids data leakage by not using validation data in preprocessing decisions\u001b[39;00m\n\u001b[32m      6\u001b[39m anli_r1_files = [\u001b[33m\"\u001b[39m\u001b[33m../datasets/anli_r1/anli_r1_train.csv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m anli_r1_encoded = \u001b[43md_enc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43manli_r1_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessed dataset structure (TRAIN ONLY):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manli_r1_encoded[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Research\\data-for-automl\\Learn2Clean\\python-package\\learn2clean\\loading\\reader.py:459\u001b[39m, in \u001b[36mReader.train_test_split\u001b[39m\u001b[34m(self, Lpath, target_name, encoding)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(Lpath) == \u001b[32m1\u001b[39m):\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLpath\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (target_name \u001b[38;5;129;01min\u001b[39;00m df.columns):\n\u001b[32m    463\u001b[39m         df_train, df_test, y_train, y_test = train_test_split(\n\u001b[32m    464\u001b[39m             df, df[target_name], test_size=\u001b[32m0.33\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Research\\data-for-automl\\Learn2Clean\\python-package\\learn2clean\\loading\\reader.py:325\u001b[39m, in \u001b[36mReader.profile\u001b[39m\u001b[34m(self, path, plot)\u001b[39m\n\u001b[32m    321\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReading csv : \u001b[39m\u001b[33m\"\u001b[39m + path.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m         df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                         \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                         \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mISO-8859-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (type_doc == \u001b[33m'\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.verbose):\n",
      "\u001b[31mTypeError\u001b[39m: read_csv() got an unexpected keyword argument 'error_bad_lines'"
     ]
    }
   ],
   "source": [
    "# Create Learn2Clean reader with encoding for text classification\n",
    "d_enc = rd.Reader(sep=',', verbose=True, encoding=True) \n",
    "\n",
    "# Process ANLI R1 dataset - ONLY TRAIN DATA for Learn2Clean optimization\n",
    "# This avoids data leakage by not using validation data in preprocessing decisions\n",
    "anli_r1_files = [\"../datasets/anli_r1/anli_r1_train.csv\"]\n",
    "anli_r1_encoded = d_enc.train_test_split(anli_r1_files, 'label')\n",
    "\n",
    "print(\"\\nProcessed dataset structure (TRAIN ONLY):\")\n",
    "print(f\"Train shape: {anli_r1_encoded['train'].shape}\")\n",
    "print(f\"Target shape: {anli_r1_encoded['target'].shape}\")\n",
    "print(f\"Target name: {anli_r1_encoded['target'].name}\")\n",
    "print(\"\\nNote: Only training data used for Learn2Clean to avoid data leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Manual Data Cleaning Pipeline\n",
    "\n",
    "Let's create a manual preprocessing pipeline for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Learn2Clean to Python path (if not already done)\n",
    "import sys\n",
    "import os\n",
    "if '../python-package' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../python-package'))\n",
    "\n",
    "# Import Learn2Clean modules for manual pipeline\n",
    "import learn2clean.loading.reader as rd \n",
    "import learn2clean.normalization.normalizer as nl \n",
    "import learn2clean.feature_selection.feature_selector as fs\n",
    "import learn2clean.duplicate_detection.duplicate_detector as dd\n",
    "import learn2clean.outlier_detection.outlier_detector as od\n",
    "import learn2clean.imputation.imputer as imp\n",
    "import learn2clean.classification.classifier as cl\n",
    "\n",
    "# Create a copy of the dataset for manual processing\n",
    "manual_dataset = anli_r1_encoded.copy()\n",
    "\n",
    "print(\"Starting manual preprocessing pipeline...\")\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "print(\"\\n1. Imputation - Replace missing values\")\n",
    "imputer = imp.Imputer(dataset=manual_dataset, strategy='median', verbose=True)\n",
    "manual_dataset = imputer.transform()\n",
    "\n",
    "# Step 2: Duplicate detection\n",
    "print(\"\\n2. Duplicate Detection\")\n",
    "dup_detector = dd.Duplicate_detector(dataset=manual_dataset, strategy='drop_duplicates', verbose=True)\n",
    "manual_dataset = dup_detector.transform()\n",
    "\n",
    "# Step 3: Feature selection for text data\n",
    "print(\"\\n3. Feature Selection\")\n",
    "feat_selector = fs.Feature_selector(dataset=manual_dataset, strategy='WR', exclude='label', verbose=True)\n",
    "manual_dataset = feat_selector.transform()\n",
    "\n",
    "print(\"\\nManual preprocessing completed!\")\n",
    "print(f\"Final train shape: {manual_dataset['train'].shape}\")\n",
    "print(f\"Final test shape: {manual_dataset['test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Classification with Manual Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification with manually cleaned data\n",
    "print(\"Testing classification with manually cleaned data...\")\n",
    "\n",
    "# Try different classifiers\n",
    "classifiers = ['CART', 'NB', 'LDA']\n",
    "\n",
    "for clf_name in classifiers:\n",
    "    try:\n",
    "        print(f\"\\nTesting {clf_name} classifier:\")\n",
    "        classifier = cl.Classifier(dataset=manual_dataset, goal=clf_name, target_goal='label', verbose=True)\n",
    "        result = classifier.transform()\n",
    "        print(f\"{clf_name} classification completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {clf_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Automated Learn2Clean Pipeline\n",
    "\n",
    "Now let's use Learn2Clean's Q-learning approach to automatically find the best preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Learn2Clean to Python path (if not already done)\n",
    "import sys\n",
    "import os\n",
    "if '../python-package' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../python-package'))\n",
    "\n",
    "import learn2clean.qlearning.qlearner as ql\n",
    "\n",
    "# Create a fresh copy of the dataset for Learn2Clean\n",
    "l2c_dataset = anli_r1_encoded.copy()\n",
    "\n",
    "print(\"Starting Learn2Clean automated pipeline...\")\n",
    "print(\"This may take several minutes to find the optimal preprocessing sequence.\")\n",
    "\n",
    "# Learn2Clean for CART classification\n",
    "l2c_classification = ql.Qlearner(\n",
    "    dataset=l2c_dataset,\n",
    "    goal='CART', \n",
    "    target_goal='label',\n",
    "    threshold=0.6, \n",
    "    target_prepare=None, \n",
    "    file_name='anli_r1_example', \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run Learn2Clean optimization\n",
    "l2c_classification.learn2clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Random Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with random preprocessing pipeline\n",
    "random_dataset = anli_r1_encoded.copy()\n",
    "\n",
    "print(\"Running random preprocessing pipeline for comparison...\")\n",
    "\n",
    "# Random preprocessing pipeline for CART classification\n",
    "random_pipeline = ql.Qlearner(\n",
    "    dataset=random_dataset,\n",
    "    goal='CART',\n",
    "    target_goal='label',\n",
    "    target_prepare=None, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    random_pipeline.random_cleaning('anli_r1_random_example')\n",
    "    print(\"Random pipeline completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Random pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Results Analysis\n",
    "\n",
    "The results of Learn2Clean and random cleaning are stored in the 'save' directory as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if results files exist and display them\n",
    "import os\n",
    "\n",
    "results_files = [\n",
    "    'save/anli_r1_example_results.txt',\n",
    "    'save/anli_r1_random_example_results_file.txt'\n",
    "]\n",
    "\n",
    "for file_path in results_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n=== Results from {file_path} ===\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(content[-500:])  # Show last 500 characters\n",
    "    else:\n",
    "        print(f\"Results file not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Applying Learned Preprocessing to Validation Data\n",
    "\n",
    "After Learn2Clean finds the optimal preprocessing pipeline on training data, we need to apply the same transformations to validation data for AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data separately\n",
    "val_data = read_dataset('anli_r1_val')\n",
    "test_data = read_dataset('anli_r1_test')\n",
    "\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# TODO: Apply the optimal preprocessing pipeline found by Learn2Clean\n",
    "# to validation and test data using the same transformations\n",
    "# (same imputation values, same normalization parameters, etc.)\n",
    "\n",
    "print(\"\\nFor AutoGluon training:\")\n",
    "print(\"1. Use the Learn2Clean optimized training data\")\n",
    "print(\"2. Apply the SAME preprocessing pipeline to validation data\")\n",
    "print(\"3. Keep test data completely separate until final evaluation\")\n",
    "print(\"4. This ensures no data leakage and valid model evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to apply Learn2Clean to the ANLI R1 dataset for natural language inference classification **while avoiding data leakage**. The key steps were:\n",
    "\n",
    "1. **Data Loading**: Loaded the ANLI R1 dataset and prepared it for text classification\n",
    "2. **Data Separation**: Kept train/validation/test splits separate to avoid data leakage\n",
    "3. **Profiling**: Used Learn2Clean's profiling capabilities to understand the data\n",
    "4. **Manual Pipeline**: Created a manual preprocessing pipeline with imputation, duplicate detection, and feature selection\n",
    "5. **Automated Pipeline**: Used Learn2Clean's Q-learning approach on TRAINING DATA ONLY\n",
    "6. **Comparison**: Compared Learn2Clean results with random preprocessing baselines\n",
    "7. **Validation Preparation**: Prepared to apply learned preprocessing to validation data\n",
    "\n",
    "**Critical for AutoGluon**: The preprocessing pipeline learned on training data must be applied to validation data using the same parameters (same imputation values, normalization statistics, etc.) to ensure valid evaluation and avoid data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
