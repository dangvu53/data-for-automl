{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bf379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn2Clean components loaded successfully\n",
      "Initial number of rows: 801\n",
      "After deduplication: Number of rows: 800\n",
      "Initial number of rows: 800\n",
      "After deduplication: Number of rows: 799\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n",
      "Initial number of rows: 2000\n",
      "After deduplication: Number of rows: 1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 23:42:37,341 - INFO - ================================================================================\n",
      "2025-07-27 23:42:37,341 - INFO - FAST FIXED META-LEARNING EXPERIMENT - ANLI R1 ONLY\n",
      "2025-07-27 23:42:37,341 - INFO - Start time: 2025-07-27 23:42:37\n",
      "2025-07-27 23:42:37,341 - INFO - ================================================================================\n",
      "2025-07-27 23:42:37,341 - INFO - Loading ANLI R1 dataset...\n",
      "2025-07-28 00:09:53,089 - INFO - ANLI R1 loaded in 27m 15.7s\n",
      "2025-07-28 00:09:53,094 - INFO - Sizes - Train: 16946, Val: 1000, Test: 1000\n",
      "2025-07-28 00:09:53,099 - INFO - \n",
      "============================================================\n",
      "2025-07-28 00:09:53,100 - INFO - PHASE 1: HYBRID EXPLORATORY META-LEARNING (Training data only)\n",
      "2025-07-28 00:09:53,100 - INFO - ============================================================\n",
      "2025-07-28 00:09:53,142 - INFO - Using 2000 training samples for meta-learning\n",
      "2025-07-28 00:09:53,142 - INFO - Starting meta-learning...\n",
      "2025-07-28 00:09:53,151 - INFO - Generation 1/5\n",
      "2025-07-28 00:09:54,137 - INFO - Pipeline 1: fitness=0.3854, retention=0.40 (799/2000)\n",
      "2025-07-28 00:09:54,137 - INFO - 🏆 New best fitness: 0.3854 (retention: 0.40)\n",
      "2025-07-28 00:09:54,322 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:09:54,322 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:09:54,511 - INFO - Filtering: 2000 -> 1640 (-360 samples)\n",
      "2025-07-28 00:09:54,778 - INFO - Learned difficulty threshold: 0.471\n",
      "2025-07-28 00:09:54,854 - INFO - Selection (balanced): 1640 -> 1526 (-114 samples)\n",
      "2025-07-28 00:09:55,166 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:09:55,166 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:09:55,466 - INFO - Filtering: 2000 -> 1638 (-362 samples)\n",
      "2025-07-28 00:09:55,661 - INFO - Learned difficulty threshold: 0.475\n",
      "2025-07-28 00:09:55,830 - INFO - Selection (balanced): 1638 -> 1533 (-105 samples)\n",
      "2025-07-28 00:09:55,830 - INFO - Pipeline 2: fitness=0.5117, retention=0.77 (1533/2000)\n",
      "2025-07-28 00:09:55,830 - INFO - 🏆 New best fitness: 0.5117 (retention: 0.77)\n",
      "2025-07-28 00:09:57,406 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:09:58,214 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:00,017 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:00,686 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:00,686 - INFO - Pipeline 3: fitness=0.5731, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:00,686 - INFO - 🏆 New best fitness: 0.5731 (retention: 1.11)\n",
      "2025-07-28 00:10:00,903 - INFO - Learned length thresholds: 352 - 477\n",
      "2025-07-28 00:10:00,903 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:01,085 - INFO - Filtering: 2000 -> 1361 (-639 samples)\n",
      "2025-07-28 00:10:01,193 - INFO - Augmentation: 1361 -> 1480 (+119 samples)\n",
      "2025-07-28 00:10:01,702 - INFO - Learned length thresholds: 352 - 477\n",
      "2025-07-28 00:10:01,702 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:01,825 - INFO - Filtering: 2000 -> 1366 (-634 samples)\n",
      "2025-07-28 00:10:01,846 - INFO - Augmentation: 1366 -> 1486 (+120 samples)\n",
      "2025-07-28 00:10:01,846 - INFO - Pipeline 4: fitness=0.5058, retention=0.74 (1486/2000)\n",
      "2025-07-28 00:10:02,003 - INFO - Learned length thresholds: 343 - 467\n",
      "2025-07-28 00:10:02,003 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:02,268 - INFO - Filtering: 2000 -> 1379 (-621 samples)\n",
      "2025-07-28 00:10:02,477 - INFO - Learned length thresholds: 343 - 468\n",
      "2025-07-28 00:10:02,477 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:02,666 - INFO - Filtering: 2000 -> 1382 (-618 samples)\n",
      "2025-07-28 00:10:02,667 - INFO - Pipeline 5: fitness=0.4825, retention=0.69 (1382/2000)\n",
      "2025-07-28 00:10:03,948 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:03,948 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:04,282 - INFO - Filtering: 1998 -> 1344 (-654 samples)\n",
      "2025-07-28 00:10:05,388 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:05,388 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:05,530 - INFO - Filtering: 1998 -> 1345 (-653 samples)\n",
      "2025-07-28 00:10:05,531 - INFO - Pipeline 6: fitness=0.4996, retention=0.67 (1345/2000)\n",
      "2025-07-28 00:10:05,732 - INFO - Learned length thresholds: 331 - 467\n",
      "2025-07-28 00:10:05,749 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:05,951 - INFO - Filtering: 2000 -> 1307 (-693 samples)\n",
      "2025-07-28 00:10:06,026 - INFO - Augmentation: 1307 -> 1421 (+114 samples)\n",
      "2025-07-28 00:10:06,431 - INFO - Learned length thresholds: 331 - 468\n",
      "2025-07-28 00:10:06,431 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:06,781 - INFO - Filtering: 2000 -> 1310 (-690 samples)\n",
      "2025-07-28 00:10:06,800 - INFO - Augmentation: 1310 -> 1425 (+115 samples)\n",
      "2025-07-28 00:10:06,802 - INFO - Pipeline 7: fitness=0.4968, retention=0.71 (1425/2000)\n",
      "2025-07-28 00:10:06,957 - INFO - Learned length thresholds: 343 - 467\n",
      "2025-07-28 00:10:06,957 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:07,074 - INFO - Filtering: 2000 -> 1379 (-621 samples)\n",
      "2025-07-28 00:10:07,244 - INFO - Learned length thresholds: 343 - 468\n",
      "2025-07-28 00:10:07,244 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:07,591 - INFO - Filtering: 2000 -> 1382 (-618 samples)\n",
      "2025-07-28 00:10:07,592 - INFO - Pipeline 8: fitness=0.4825, retention=0.69 (1382/2000)\n",
      "2025-07-28 00:10:07,594 - INFO - Generation 1 completed in 14.44s - Best: 0.5731, Avg: 0.4922\n",
      "2025-07-28 00:10:07,596 - INFO - Generation 2/5\n",
      "2025-07-28 00:10:08,688 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:09,188 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:11,049 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:11,503 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:11,504 - INFO - Pipeline 1: fitness=0.5752, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:11,504 - INFO - 🏆 New best fitness: 0.5752 (retention: 1.11)\n",
      "2025-07-28 00:10:11,859 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:11,860 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:12,024 - INFO - Filtering: 2000 -> 1640 (-360 samples)\n",
      "2025-07-28 00:10:12,252 - INFO - Learned difficulty threshold: 0.471\n",
      "2025-07-28 00:10:12,603 - INFO - Selection (balanced): 1640 -> 1526 (-114 samples)\n",
      "2025-07-28 00:10:13,099 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:13,099 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:13,329 - INFO - Filtering: 2000 -> 1638 (-362 samples)\n",
      "2025-07-28 00:10:13,604 - INFO - Learned difficulty threshold: 0.475\n",
      "2025-07-28 00:10:13,844 - INFO - Selection (balanced): 1638 -> 1533 (-105 samples)\n",
      "2025-07-28 00:10:13,845 - INFO - Pipeline 2: fitness=0.5117, retention=0.77 (1533/2000)\n",
      "2025-07-28 00:10:14,380 - INFO - Learned length thresholds: 352 - 477\n",
      "2025-07-28 00:10:14,380 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:14,497 - INFO - Filtering: 2000 -> 1361 (-639 samples)\n",
      "2025-07-28 00:10:14,526 - INFO - Augmentation: 1361 -> 1480 (+119 samples)\n",
      "2025-07-28 00:10:14,877 - INFO - Learned length thresholds: 352 - 477\n",
      "2025-07-28 00:10:14,877 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:15,145 - INFO - Filtering: 2000 -> 1366 (-634 samples)\n",
      "2025-07-28 00:10:15,213 - INFO - Augmentation: 1366 -> 1486 (+120 samples)\n",
      "2025-07-28 00:10:15,214 - INFO - Pipeline 3: fitness=0.5058, retention=0.74 (1486/2000)\n",
      "2025-07-28 00:10:16,323 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:16,324 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:16,553 - INFO - Filtering: 1998 -> 1344 (-654 samples)\n",
      "2025-07-28 00:10:17,360 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:17,360 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:17,724 - INFO - Filtering: 1998 -> 1345 (-653 samples)\n",
      "2025-07-28 00:10:17,724 - INFO - Pipeline 4: fitness=0.4996, retention=0.67 (1345/2000)\n",
      "2025-07-28 00:10:19,002 - INFO - Learned length thresholds: 352 - 477\n",
      "2025-07-28 00:10:19,003 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:19,226 - INFO - Filtering: 1998 -> 1359 (-639 samples)\n",
      "2025-07-28 00:10:19,282 - INFO - Augmentation: 1359 -> 1478 (+119 samples)\n",
      "2025-07-28 00:10:20,725 - INFO - Learned length thresholds: 353 - 477\n",
      "2025-07-28 00:10:20,725 - INFO - Learned quality threshold: 0.676\n",
      "2025-07-28 00:10:21,065 - INFO - Filtering: 1998 -> 1354 (-644 samples)\n",
      "2025-07-28 00:10:21,084 - INFO - Augmentation: 1354 -> 1473 (+119 samples)\n",
      "2025-07-28 00:10:21,084 - INFO - Pipeline 5: fitness=0.4960, retention=0.74 (1473/2000)\n",
      "2025-07-28 00:10:22,653 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:22,662 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:24,220 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:24,263 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:24,264 - INFO - Pipeline 6: fitness=0.5762, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:24,265 - INFO - 🏆 New best fitness: 0.5762 (retention: 1.11)\n",
      "2025-07-28 00:10:24,761 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:24,761 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:24,905 - INFO - Filtering: 2000 -> 1640 (-360 samples)\n",
      "2025-07-28 00:10:24,998 - INFO - Learned difficulty threshold: 0.471\n",
      "2025-07-28 00:10:25,105 - INFO - Selection (balanced): 1640 -> 1526 (-114 samples)\n",
      "2025-07-28 00:10:25,672 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:25,673 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:26,026 - INFO - Filtering: 2000 -> 1638 (-362 samples)\n",
      "2025-07-28 00:10:26,234 - INFO - Learned difficulty threshold: 0.475\n",
      "2025-07-28 00:10:26,526 - INFO - Selection (balanced): 1638 -> 1533 (-105 samples)\n",
      "2025-07-28 00:10:26,527 - INFO - Pipeline 7: fitness=0.5117, retention=0.77 (1533/2000)\n",
      "2025-07-28 00:10:27,589 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:27,589 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:27,710 - INFO - Filtering: 1998 -> 1344 (-654 samples)\n",
      "2025-07-28 00:10:28,263 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:28,263 - INFO - Learned quality threshold: 0.685\n",
      "2025-07-28 00:10:28,405 - INFO - Filtering: 1998 -> 1345 (-653 samples)\n",
      "2025-07-28 00:10:28,416 - INFO - Pipeline 8: fitness=0.0000, retention=0.27 (538/2000)\n",
      "2025-07-28 00:10:28,416 - INFO - Generation 2 completed in 20.82s - Best: 0.5762, Avg: 0.4595\n",
      "2025-07-28 00:10:28,416 - INFO - Generation 3/5\n",
      "2025-07-28 00:10:30,092 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:30,099 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:31,509 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:31,517 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:31,517 - INFO - Pipeline 1: fitness=0.5762, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:32,763 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:33,363 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:34,816 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:35,406 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:35,406 - INFO - Pipeline 2: fitness=0.5689, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:35,790 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:35,791 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:36,041 - INFO - Filtering: 2000 -> 1640 (-360 samples)\n",
      "2025-07-28 00:10:36,248 - INFO - Learned difficulty threshold: 0.471\n",
      "2025-07-28 00:10:36,509 - INFO - Selection (balanced): 1640 -> 1526 (-114 samples)\n",
      "2025-07-28 00:10:36,782 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:36,782 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:36,893 - INFO - Filtering: 2000 -> 1638 (-362 samples)\n",
      "2025-07-28 00:10:36,972 - INFO - Learned difficulty threshold: 0.475\n",
      "2025-07-28 00:10:37,170 - INFO - Selection (balanced): 1638 -> 1533 (-105 samples)\n",
      "2025-07-28 00:10:37,170 - INFO - Pipeline 3: fitness=0.5117, retention=0.77 (1533/2000)\n",
      "2025-07-28 00:10:37,444 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:37,444 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:37,730 - INFO - Filtering: 2000 -> 1640 (-360 samples)\n",
      "2025-07-28 00:10:38,043 - INFO - Learned difficulty threshold: 0.471\n",
      "2025-07-28 00:10:38,123 - INFO - Selection (balanced): 1640 -> 1526 (-114 samples)\n",
      "2025-07-28 00:10:38,602 - INFO - Learned length thresholds: 331 - 477\n",
      "2025-07-28 00:10:38,602 - INFO - Learned quality threshold: 0.661\n",
      "2025-07-28 00:10:40,653 - INFO - Filtering: 2000 -> 1638 (-362 samples)\n",
      "2025-07-28 00:10:40,738 - INFO - Learned difficulty threshold: 0.475\n",
      "2025-07-28 00:10:40,854 - INFO - Selection (balanced): 1638 -> 1533 (-105 samples)\n",
      "2025-07-28 00:10:40,855 - INFO - Pipeline 4: fitness=0.5117, retention=0.77 (1533/2000)\n",
      "2025-07-28 00:10:41,767 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:42,179 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:43,435 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:44,266 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:44,281 - INFO - Pipeline 5: fitness=0.4474, retention=0.56 (1115/2000)\n",
      "2025-07-28 00:10:44,916 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:45,602 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:46,208 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:46,736 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:46,736 - INFO - Pipeline 6: fitness=0.5710, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:48,012 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:49,258 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:49,259 - INFO - Pipeline 7: fitness=0.5644, retention=1.09 (2178/2000)\n",
      "2025-07-28 00:10:50,495 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:50,505 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:52,036 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:52,047 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:52,048 - INFO - Pipeline 8: fitness=0.5783, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:52,048 - INFO - 🏆 New best fitness: 0.5783 (retention: 1.11)\n",
      "2025-07-28 00:10:52,048 - INFO - Generation 3 completed in 23.63s - Best: 0.5783, Avg: 0.5412\n",
      "2025-07-28 00:10:52,048 - INFO - Generation 4/5\n",
      "2025-07-28 00:10:53,218 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:53,226 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:54,306 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:54,315 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:54,315 - INFO - Pipeline 1: fitness=0.5815, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:54,315 - INFO - 🏆 New best fitness: 0.5815 (retention: 1.11)\n",
      "2025-07-28 00:10:55,188 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:55,197 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:56,571 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:56,580 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:56,580 - INFO - Pipeline 2: fitness=0.5773, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:10:57,435 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:58,212 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:58,843 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:10:59,476 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:10:59,477 - INFO - Pipeline 3: fitness=0.5710, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:01,071 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:01,858 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:03,097 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:03,745 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:03,745 - INFO - Pipeline 4: fitness=0.5710, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:05,178 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:05,726 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:06,880 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:07,362 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:07,363 - INFO - Pipeline 5: fitness=0.5909, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:07,363 - INFO - 🏆 New best fitness: 0.5909 (retention: 1.11)\n",
      "2025-07-28 00:11:08,369 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:08,376 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:09,481 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:09,489 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:09,489 - INFO - Pipeline 6: fitness=0.5679, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:10,944 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:11,510 - INFO - Augmentation: 1961 -> 2007 (+46 samples)\n",
      "2025-07-28 00:11:13,390 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:14,261 - INFO - Augmentation: 1963 -> 2009 (+46 samples)\n",
      "2025-07-28 00:11:14,261 - INFO - Pipeline 7: fitness=0.5693, retention=1.00 (2009/2000)\n",
      "2025-07-28 00:11:15,193 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:15,213 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:16,053 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:16,060 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:16,061 - INFO - Pipeline 8: fitness=0.5741, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:16,061 - INFO - Generation 4 completed in 24.01s - Best: 0.5909, Avg: 0.5754\n",
      "2025-07-28 00:11:16,061 - INFO - Generation 5/5\n",
      "2025-07-28 00:11:17,467 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:18,152 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:19,676 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:20,393 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:20,394 - INFO - Pipeline 1: fitness=0.5668, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:21,864 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:21,873 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:23,382 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:23,389 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:23,389 - INFO - Pipeline 2: fitness=0.5700, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:24,893 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:24,902 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:26,366 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:26,373 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:26,374 - INFO - Pipeline 3: fitness=0.5689, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:27,236 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:27,264 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:27,876 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:27,884 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:27,884 - INFO - Pipeline 4: fitness=0.5647, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:29,745 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:30,372 - INFO - Augmentation: 2166 -> 2216 (+50 samples)\n",
      "2025-07-28 00:11:32,225 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:33,136 - INFO - Augmentation: 2166 -> 2216 (+50 samples)\n",
      "2025-07-28 00:11:33,136 - INFO - Pipeline 5: fitness=0.5842, retention=1.11 (2216/2000)\n",
      "2025-07-28 00:11:33,203 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:33,211 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:33,292 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:33,300 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:33,300 - INFO - Pipeline 6: fitness=0.5731, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:35,208 - INFO - Augmentation: 2000 -> 2044 (+44 samples)\n",
      "2025-07-28 00:11:37,426 - INFO - Augmentation: 2000 -> 2044 (+44 samples)\n",
      "2025-07-28 00:11:37,427 - INFO - Pipeline 7: fitness=0.5702, retention=1.02 (2044/2000)\n",
      "2025-07-28 00:11:37,939 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:37,947 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:38,943 - INFO - Augmentation: 2000 -> 2178 (+178 samples)\n",
      "2025-07-28 00:11:38,952 - INFO - Augmentation: 2178 -> 2228 (+50 samples)\n",
      "2025-07-28 00:11:39,628 - INFO - Pipeline 8: fitness=0.5836, retention=1.11 (2228/2000)\n",
      "2025-07-28 00:11:39,628 - INFO - Generation 5 completed in 23.57s - Best: 0.5842, Avg: 0.5727\n",
      "2025-07-28 00:11:39,628 - INFO - Meta-learning completed in 1m 46.5s\n",
      "2025-07-28 00:11:39,628 - INFO - Best fitness found: 0.5909\n",
      "2025-07-28 00:11:39,628 - INFO - \n",
      "============================================================\n",
      "2025-07-28 00:11:39,628 - INFO - PHASE 2: APPLYING DISCOVERED PIPELINE\n",
      "2025-07-28 00:11:39,628 - INFO - ============================================================\n",
      "2025-07-28 00:11:39,628 - INFO - Best pipeline configuration:\n",
      "2025-07-28 00:11:39,628 - INFO -   Approach: random\n",
      "2025-07-28 00:11:39,628 - INFO -   Operations:\n",
      "2025-07-28 00:11:39,654 - INFO -     1. advanced_outlier_zscore: {'strategy': 'ZSB', 'threshold': 0.3, 'tfidf_max_features': 150, 'svd_components': 20}\n",
      "2025-07-28 00:11:39,655 - INFO -     2. advanced_dedup_metric: {'strategy': 'METRIC', 'metric': 'DL', 'threshold': 0.6, 'tfidf_max_features': 200, 'svd_components': 20}\n",
      "2025-07-28 00:11:39,655 - INFO -     3. class_balance_augment: {'target_ratio': 1.0}\n",
      "2025-07-28 00:11:39,655 - INFO -     4. advanced_outlier_iqr: {'strategy': 'IQR', 'threshold': 0.1, 'tfidf_max_features': 200, 'svd_components': 15}\n",
      "2025-07-28 00:11:39,655 - INFO -     5. difficulty_select_balanced: {'threshold_percentile': 50, 'range': 0.4}\n",
      "2025-07-28 00:11:39,655 - INFO -     6. deletion_augment: {'ratio': 0.03, 'minority_boost': 1.5}\n",
      "2025-07-28 00:11:39,655 - INFO - Applying mandatory text cleaning to all datasets...\n",
      "2025-07-28 00:11:40,310 - INFO - Cleaning completed:\n",
      "2025-07-28 00:11:40,310 - INFO -   Train: 16946 -> 16946 (-0 samples)\n",
      "2025-07-28 00:11:40,310 - INFO -   Val: 1000 -> 1000 (-0 samples)\n",
      "2025-07-28 00:11:40,310 - INFO -   Test: 1000 -> 1000 (-0 samples)\n",
      "2025-07-28 00:11:40,310 - INFO - Applying discovered pipeline to cleaned training data...\n",
      "2025-07-28 00:11:49,343 - INFO - Augmentation: 16946 -> 18470 (+1524 samples)\n",
      "2025-07-28 00:11:54,329 - INFO - Augmentation: 18468 -> 18907 (+439 samples)\n",
      "2025-07-28 00:11:54,331 - INFO - Data processing completed in 14.68 seconds\n",
      "2025-07-28 00:11:54,331 - INFO - Data sizes after processing:\n",
      "2025-07-28 00:11:54,339 - INFO -   Train: 16946 -> 18907 (111.6%)\n",
      "2025-07-28 00:11:54,339 - INFO -     (Cleaning: 16946 -> 16946, Pipeline: 16946 -> 18907)\n",
      "2025-07-28 00:11:54,339 - INFO -   Val: 1000 -> 1000 (100.0%) [cleaning only]\n",
      "2025-07-28 00:11:54,339 - INFO -   Test: 1000 -> 1000 (100.0%) [cleaning only]\n",
      "2025-07-28 00:11:54,339 - INFO - \n",
      "============================================================\n",
      "2025-07-28 00:11:54,340 - INFO - PHASE 3: AUTOGLUON TRAINING (Your exact settings)\n",
      "2025-07-28 00:11:54,340 - INFO - ============================================================\n",
      "2025-07-28 00:11:54,340 - INFO - Using TabularPredictor for text data for anli_r1\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./anli_r1_model\"\n",
      "2025-07-28 00:11:54,342 - INFO - Using AutoGluon's default models and settings\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.11.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          12\n",
      "Memory Avail:       0.47 GB / 7.71 GB (6.1%)\n",
      "Disk Space Avail:   215.99 GB / 312.50 GB (69.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 3000s\n",
      "AutoGluon will save models to \"d:\\Research\\data-for-automl\\anli_r1_model\"\n",
      "Train Data Rows:    18907\n",
      "Train Data Columns: 3\n",
      "Tuning Data Rows:    1000\n",
      "Tuning Data Columns: 3\n",
      "Label Column:       label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    475.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.39 MB (4.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text', 'premise', 'hypothesis']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 10000 to 598 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['text']) : 3 | ['text', 'premise', 'hypothesis']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', ['text_as_category'])  :   3 | ['text', 'premise', 'hypothesis']\n",
      "\t\t('int', ['binned', 'text_special']) :  52 | ['text.char_count', 'text.word_count', 'text.lower_ratio', 'text.digit_ratio', 'text.special_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 595 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', ...]\n",
      "\t51.8s = Fit runtime\n",
      "\t3 features in original data used to generate 650 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 23.69 MB (4.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 52.34s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 2947.65s of the 2947.65s of remaining time.\n",
      "\t0.314\t = Validation score   (accuracy)\n",
      "\t15.19s\t = Training   runtime\n",
      "\t7.46s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 2924.93s of the 2924.92s of remaining time.\n",
      "\t0.326\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 2924.10s of the 2924.10s of remaining time.\n",
      "\t0.374\t = Validation score   (accuracy)\n",
      "\t83.87s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 2839.99s of the 2839.99s of remaining time.\n",
      "\t0.376\t = Validation score   (accuracy)\n",
      "\t4.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2835.24s of the 2835.23s of remaining time.\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t7.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2828.03s of the 2828.03s of remaining time.\n",
      "\t0.361\t = Validation score   (accuracy)\n",
      "\t11.09s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 2815.88s of the 2815.88s of remaining time.\n",
      "\t0.352\t = Validation score   (accuracy)\n",
      "\t9.41s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 2805.32s of the 2805.31s of remaining time.\n",
      "\t0.341\t = Validation score   (accuracy)\n",
      "\t31.0s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 2774.23s of the 2774.22s of remaining time.\n",
      "\t0.356\t = Validation score   (accuracy)\n",
      "\t11.3s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 2761.80s of the 2761.80s of remaining time.\n",
      "\t0.362\t = Validation score   (accuracy)\n",
      "\t12.0s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 2748.63s of the 2748.63s of remaining time.\n",
      "\t0.367\t = Validation score   (accuracy)\n",
      "\t9.79s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 2738.78s of the 2738.78s of remaining time.\n",
      "\t0.365\t = Validation score   (accuracy)\n",
      "\t80.89s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2657.77s of the 2657.77s of remaining time.\n",
      "Warning: Low available memory may cause OOM error if training continues\n",
      "Available Memory: 503 MB\n",
      "Estimated GBM model size: 7 MB\n",
      "Warning: Early stopped GBM model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\t0.352\t = Validation score   (accuracy)\n",
      "\t6.03s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 2651.37s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.8, 'LightGBMXT': 0.2}\n",
      "\t0.384\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 349.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6341.9 rows/s (1000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\Research\\data-for-automl\\anli_r1_model\")\n",
      "2025-07-28 00:17:44,820 - INFO - Text model training completed for anli_r1 in 5m 50.5s\n",
      "2025-07-28 00:17:44,820 - INFO - \n",
      "============================================================\n",
      "2025-07-28 00:17:44,820 - INFO - PHASE 4: EVALUATION\n",
      "2025-07-28 00:17:44,820 - INFO - ============================================================\n",
      "2025-07-28 00:17:54,073 - INFO - Evaluation completed in 9.25 seconds\n",
      "2025-07-28 00:17:54,076 - INFO - Test performance: {'accuracy': 0.376, 'balanced_accuracy': 0.3760556964149779, 'mcc': 0.06472115074827779}\n",
      "2025-07-28 00:17:54,077 - INFO - Best model: WeightedEnsemble_L2\n",
      "2025-07-28 00:17:54,078 - INFO - Test score: 0.376\n",
      "2025-07-28 00:17:54,078 - INFO - \n",
      "================================================================================\n",
      "2025-07-28 00:17:54,078 - INFO - EXPERIMENT SUMMARY\n",
      "2025-07-28 00:17:54,078 - INFO - ================================================================================\n",
      "2025-07-28 00:17:54,081 - INFO - Meta-learning fitness: 0.5909 (70% accuracy + 30% retention)\n",
      "2025-07-28 00:17:54,082 - INFO - Data retention: 16946 -> 18907 (111.6%)\n",
      "2025-07-28 00:17:54,082 - INFO - ✅ Good data retention - pipeline preserves most training data\n",
      "2025-07-28 00:17:54,083 - INFO - AutoGluon performance: {'accuracy': 0.376, 'balanced_accuracy': 0.3760556964149779, 'mcc': 0.06472115074827779}\n",
      "2025-07-28 00:17:54,083 - INFO - Processing time: 14.68 seconds\n",
      "2025-07-28 00:17:54,083 - INFO - AutoGluon time: 5m 50.5s\n",
      "2025-07-28 00:17:54,083 - INFO - Total time: 35m 16.7s\n",
      "2025-07-28 00:17:54,083 - INFO - ================================================================================\n",
      "2025-07-28 00:17:54,089 - INFO - Results saved to fast_experiment_results.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!python fast_fixed_meta_learning_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "TRAIN_FILE = \"train.csv\"  # Update with your actual train file name\n",
    "TEST_FILE = \"test.csv\"    # Update with your actual test file name\n",
    "MODEL_PATH = \"autogluon_models\"\n",
    "TARGET_COLUMN = \"label\"   # Update with your actual target column name\n",
    "\n",
    "def run_autogluon_experiment():\n",
    "    \"\"\"Run AutoGluon experiment on saved CSV files\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting AutoGluon Experiment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(TRAIN_FILE):\n",
    "        print(f\"❌ Train file '{TRAIN_FILE}' not found!\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(TEST_FILE):\n",
    "        print(f\"❌ Test file '{TEST_FILE}' not found!\")\n",
    "        return\n",
    "    \n",
    "    # Load data\n",
    "    print(\"📂 Loading data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_data = pd.read_csv(TRAIN_FILE)\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"✅ Data loaded successfully in {load_time:.2f} seconds\")\n",
    "    print(f\"   Train shape: {train_data.shape}\")\n",
    "    print(f\"   Test shape: {test_data.shape}\")\n",
    "    \n",
    "    # Check target column\n",
    "    if TARGET_COLUMN not in train_data.columns:\n",
    "        print(f\"❌ Target column '{TARGET_COLUMN}' not found in train data!\")\n",
    "        print(f\"Available columns: {list(train_data.columns)}\")\n",
    "        return\n",
    "    \n",
    "    # Display data info\n",
    "    print(\"\\n📊 Dataset Information:\")\n",
    "    print(f\"   Target column: {TARGET_COLUMN}\")\n",
    "    print(f\"   Target distribution:\")\n",
    "    print(train_data[TARGET_COLUMN].value_counts().to_string())\n",
    "    \n",
    "    # Check for text columns\n",
    "    text_columns = [col for col in train_data.columns if train_data[col].dtype == 'object' and col != TARGET_COLUMN]\n",
    "    print(f\"   Text columns: {text_columns}\")\n",
    "    \n",
    "    # Setup AutoGluon predictor\n",
    "    print(\"\\n🤖 Setting up AutoGluon TabularPredictor...\")\n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET_COLUMN,\n",
    "        path=MODEL_PATH,\n",
    "        problem_type='classification',  # Change to 'regression' if needed\n",
    "        eval_metric='accuracy',         # Change metric if needed\n",
    "        verbosity=2\n",
    "    )\n",
    "    \n",
    "    # Configure training parameters\n",
    "    time_limit = 600  # 10 minutes - adjust as needed\n",
    "    presets = 'best_quality'  # Options: 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n",
    "    \n",
    "    print(f\"   Time limit: {time_limit} seconds\")\n",
    "    print(f\"   Presets: {presets}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\n🏋️ Training AutoGluon model (started at {datetime.now().strftime('%H:%M:%S')})...\")\n",
    "    train_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        predictor.fit(\n",
    "            train_data=train_data,\n",
    "            time_limit=time_limit,\n",
    "            presets=presets,\n",
    "            num_cpus='auto',\n",
    "            num_gpus='auto' if 'cuda' in str(train_data.device) if hasattr(train_data, 'device') else 0\n",
    "        )\n",
    "        \n",
    "        train_time = time.time() - train_start\n",
    "        print(f\"✅ Training completed in {train_time/60:.2f} minutes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get model information\n",
    "    print(\"\\n📋 Model Information:\")\n",
    "    leaderboard = predictor.leaderboard(train_data, silent=True)\n",
    "    print(\"Top 5 models:\")\n",
    "    print(leaderboard.head().to_string())\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    print(\"\\n🔮 Making predictions on test set...\")\n",
    "    pred_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Check if test data has target column (for evaluation)\n",
    "        if TARGET_COLUMN in test_data.columns:\n",
    "            # Test data has labels - we can evaluate\n",
    "            test_labels = test_data[TARGET_COLUMN]\n",
    "            test_features = test_data.drop(columns=[TARGET_COLUMN])\n",
    "            \n",
    "            predictions = predictor.predict(test_features)\n",
    "            pred_proba = predictor.predict_proba(test_features)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            from sklearn.metrics import accuracy_score, classification_report\n",
    "            accuracy = accuracy_score(test_labels, predictions)\n",
    "            \n",
    "            print(f\"✅ Test Accuracy: {accuracy:.4f}\")\n",
    "            print(\"\\n📊 Classification Report:\")\n",
    "            print(classification_report(test_labels, predictions))\n",
    "            \n",
    "        else:\n",
    "            # Test data doesn't have labels - just predict\n",
    "            predictions = predictor.predict(test_data)\n",
    "            pred_proba = predictor.predict_proba(test_data)\n",
    "            \n",
    "            print(f\"✅ Predictions generated for {len(predictions)} samples\")\n",
    "        \n",
    "        pred_time = time.time() - pred_start\n",
    "        print(f\"⏱️ Prediction time: {pred_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Prediction failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Save predictions\n",
    "    print(\"\\n💾 Saving predictions...\")\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'predictions': predictions\n",
    "    })\n",
    "    \n",
    "    # Add prediction probabilities if available\n",
    "    if pred_proba is not None and hasattr(pred_proba, 'columns'):\n",
    "        for col in pred_proba.columns:\n",
    "            predictions_df[f'prob_{col}'] = pred_proba[col]\n",
    "    \n",
    "    predictions_file = f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    predictions_df.to_csv(predictions_file, index=False)\n",
    "    print(f\"✅ Predictions saved to: {predictions_file}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    try:\n",
    "        print(\"\\n🎯 Feature Importance:\")\n",
    "        feature_importance = predictor.feature_importance(train_data)\n",
    "        print(\"Top 10 most important features:\")\n",
    "        print(feature_importance.head(10).to_string())\n",
    "        \n",
    "        # Save feature importance\n",
    "        importance_file = f\"feature_importance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        feature_importance.to_csv(importance_file)\n",
    "        print(f\"✅ Feature importance saved to: {importance_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not compute feature importance: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎉 AutoGluon Experiment Summary:\")\n",
    "    print(f\"   Total time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"   Training time: {train_time/60:.2f} minutes\")\n",
    "    print(f\"   Model path: {MODEL_PATH}\")\n",
    "    print(f\"   Predictions file: {predictions_file}\")\n",
    "    if TARGET_COLUMN in test_data.columns:\n",
    "        print(f\"   Test accuracy: {accuracy:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the experiment\n",
    "run_autogluon_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick AutoGluon Configuration - Update these variables as needed\n",
    "TRAIN_FILE = \"train.csv\"      # Your train file name\n",
    "TEST_FILE = \"test.csv\"        # Your test file name  \n",
    "TARGET_COLUMN = \"label\"       # Your target column name\n",
    "TIME_LIMIT = 300             # Training time in seconds (5 minutes)\n",
    "PRESETS = 'medium_quality'   # Options: 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"  Train file: {TRAIN_FILE}\")\n",
    "print(f\"  Test file: {TEST_FILE}\")\n",
    "print(f\"  Target column: {TARGET_COLUMN}\")\n",
    "print(f\"  Time limit: {TIME_LIMIT} seconds ({TIME_LIMIT/60:.1f} minutes)\")\n",
    "print(f\"  Quality preset: {PRESETS}\")\n",
    "\n",
    "# Check if files exist\n",
    "import os\n",
    "print(\"\\nFile Status:\")\n",
    "print(f\"  {TRAIN_FILE}: {'✅ Found' if os.path.exists(TRAIN_FILE) else '❌ Not found'}\")\n",
    "print(f\"  {TEST_FILE}: {'✅ Found' if os.path.exists(TEST_FILE) else '❌ Not found'}\")\n",
    "\n",
    "# Quick data preview\n",
    "if os.path.exists(TRAIN_FILE):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    print(f\"\\nTrain Data Preview:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    if TARGET_COLUMN in df.columns:\n",
    "        print(f\"  Target distribution:\")\n",
    "        print(f\"    {df[TARGET_COLUMN].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ Target column '{TARGET_COLUMN}' not found!\")\n",
    "        print(f\"  Available columns: {list(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
